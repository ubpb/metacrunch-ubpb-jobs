Bundler.require(:default, :development)

require_relative "etc/helper"
require_relative "etc/mapping"
require_relative "etc/settings"

#
# Options
#
options do
  # Database
  add :db_host, "--db-host HOSTNAME", "The database host", String, default: "localhost"
  add :db_port, "--db-port PORT", "The database port", Integer, default: 1521
  add :db_name, "--db-name NAME", "The database name", String, default: "aleph22"
  add :db_user, "--db-user USERNAME", "The database username", String, required: true
  add :db_password, "--db-password PASSWORD", "The database password", String, required: true
  # Elasticsearch
  add :es_url,   "--es-url URL", String, "Elasticsearch url", default: "localhost:9200"
  add :es_index, "--es-index STRING", String, "Elasticsearch index name", required: true
  # Loglevel
  add :log_level, "--log-level LEVEL", "The log level", String, default: "info"
  # Timestamp
  add :last_run, "--last_run DATETIME", "Only fetch records that have been updated since DATETIME", String, required: true
end

# Setup a logger
logger = Logger.new(STDOUT)
logger.level = options[:log_level]

# Setup MAB transformations
mab2primo_transformation = Metacrunch::UBPB::Transformations::MabToPrimo.new
primo2es_transformation = Metacrunch::UBPB::Transformations::PrimoToElasticsearch.new

# Setup Elasticsearch
SETTINGS[:index][:number_of_replicas] = 0
es_index_creator = Metacrunch::Elasticsearch::IndexCreator.new({
  delete_existing_index: false,
  default_mapping: MAPPING,
  settings: SETTINGS,
  url: options[:es_url],
  index: options[:es_index],
  logger: logger
})
es_indexer = Metacrunch::Elasticsearch::Indexer.new({
  id_accessor: -> (item) { item["id"] },
  url: options[:es_url],
  index: options[:es_index],
  type: "aleph_record",
  logger: logger
})

# Setup Oracle connection.
# If you need an ssh tunnel use: ssh -L 1521:localhost:1521 your_oracle_box
DB = Sequel.oracle(
  options[:db_name], 
  user: options[:db_user], 
  password: options[:db_password], 
  host: options[:db_host], 
  port: options[:db_port], 
  logger: logger
)

# Prepare the dataset
last_run = Time.parse(options[:last_run]).utc.strftime("%Y%m%d%H%M%L")
dataset  = DB.from(Sequel[:pad50][:z00p]).order(:z00p_doc_number).where{z00p_timestamp >= last_run}

# Set the source
source Metacrunch::Db::Reader.new(DB, ->(db) {
  dataset
}, rows_per_fetch: 5000)

# Pre-process
pre_process do
  # Creates the ES index if it doesn't exists.
  es_index_creator.call
  # Log
  logger.info "Processing #{dataset.count} records."
end

# Loggin step
transformation do |data|
  logger.debug "Processing record #{data[:z00p_doc_number]}"
  data
end

# Extract ID and DATA
transformation do |data|
  {
    id: data[:z00p_doc_number],
    data: data[:z00p_str] || data[:z00p_ptr]
  }
end

# Transform mab data to our intermediate format for indexing
transformation do |data|
  mab2primo_result = mab2primo_transformation.call(data[:data])
  decode_json!(mab2primo_result)
  primo2es_transformation.call(mab2primo_result)
end

# Use bulks of 1000 records
transformation_buffer(1000)

# Write the bulks to ES
transformation do |bulk|
  es_indexer.call(bulk)
end